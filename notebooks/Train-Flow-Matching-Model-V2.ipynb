{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-enemy",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "young-value",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-helen",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tqdm\n",
    "import wandb\n",
    "from flow_matching.solver.ode_solver import ODESolver\n",
    "from skimage.metrics import peak_signal_noise_ratio\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ConstantLR\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms.v2 import Compose, Resize, ToDtype, ToImage\n",
    "\n",
    "from fmplug.engine.scaling import NativeScaler\n",
    "from fmplug.engine.trainer import train_one_epoch\n",
    "from fmplug.engine.utils import get_time_discretization\n",
    "from fmplug.models.cfg_scaled import CFGScaledModel\n",
    "from fmplug.models.utils import instantiate_model\n",
    "from fmplug.utils.image_utils import mask_generator\n",
    "from fmplug.utils.measurements import get_noise, get_operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-retailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-twist",
   "metadata": {},
   "source": [
    "# Load The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-parts",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"celeba\"\n",
    "download = False\n",
    "\n",
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    Resize((128, 128)),\n",
    "    ToImage(),\n",
    "    ToDtype(torch.float32, scale=True),\n",
    "])\n",
    "\n",
    "\n",
    "if dataset == \"celeba\":\n",
    "    train_dataset = datasets.CelebA(\n",
    "        '/home/jusun/dever120/datasets/celeba',\n",
    "        split=\"train\",\n",
    "        download=download,\n",
    "        transform=transform\n",
    "    )\n",
    "    test_dataset = datasets.CelebA(\n",
    "        '/home/jusun/dever120/datasets/celeba',\n",
    "        split=\"test\",\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "elif dataset == \"cifar10\":\n",
    "    train_dataset = datasets.CIFAR10(\n",
    "        '../data',\n",
    "        train=True,\n",
    "        download=download,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    test_dataset = datasets.CIFAR10(\n",
    "        '../data',\n",
    "        train=False,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-opportunity",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-bloom",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = train_data[0]\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-winter",
   "metadata": {},
   "outputs": [],
   "source": [
    "images.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-philippines",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a single image to view\n",
    "image = (\n",
    "    images[0, :, :, :]\n",
    "    .squeeze()\n",
    "    .numpy()\n",
    "    .transpose(2, 1, 0)\n",
    "    .transpose(1, 0, 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-mechanism",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-nickname",
   "metadata": {},
   "source": [
    "# Initialize W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-formula",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables for wandb\n",
    "API_KEY = \"2080070c4753d0384b073105ed75e1f46669e4bf\"\n",
    "PROJECT_NAME = \"FMPlug\"\n",
    "\n",
    "\n",
    "# Enable wandb\n",
    "print(\"Initialize Project ...\")\n",
    "wandb.login(key=API_KEY)  # type: ignore\n",
    "\n",
    "wandb_instance = wandb.init(  # type: ignore\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=PROJECT_NAME,\n",
    "    tags=[\"Experimental\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-manor",
   "metadata": {},
   "source": [
    "# Train the Flow Matching Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-visitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = instantiate_model(\n",
    "    architechture=dataset,\n",
    "    is_discrete=False,\n",
    "    use_ema=True,\n",
    ")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-uncle",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total steps {len(train_loader)}\")\n",
    "print(f\"Trainable parameters {count_trainable_parameters(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-patch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs\n",
    "epochs = 1\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=1e-4,\n",
    "    eps=1e-8,\n",
    ")\n",
    "\n",
    "# Initalize the learning rate scheduler\n",
    "lr_schedule = torch.optim.lr_scheduler.ConstantLR(\n",
    "    optimizer, total_iters=(len(train_loader) * epochs), factor=1.0\n",
    ")\n",
    "\n",
    "# Initialize the scaler used for mixed precision\n",
    "loss_scaler = NativeScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-making",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(0, epochs):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    train_stats = train_one_epoch(\n",
    "        model=model,\n",
    "        data_loader=train_loader,\n",
    "        optimizer=optimizer,\n",
    "        lr_schedule=lr_schedule,\n",
    "        device=device,\n",
    "        epoch=epoch,\n",
    "        loss_scaler=loss_scaler,\n",
    "    )\n",
    "    \n",
    "    log_stats = {\n",
    "        **{f\"train_{k}\": v for k, v in train_stats.items()},\n",
    "        \"epoch\": epoch,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-treasury",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "torch.save(model.state_dict(), '/home/jusun/dever120/FMPlug/models/celeba-fm-model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-composition",
   "metadata": {},
   "source": [
    "# Start From Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-sociology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dataset\n",
    "# dataset = \"celeba\"\n",
    "# device = torch.device(\"cuda\")\n",
    "\n",
    "# # Load a model\n",
    "# model = instantiate_model(\n",
    "#     architechture=dataset,\n",
    "#     is_discrete=False,\n",
    "#     use_ema=True,\n",
    "# )\n",
    "# model = torch.load('/home/jusun/dever120/FMPlug/models/celeba-fm-model.pt')\n",
    "# model = model.to(device)\n",
    "\n",
    "# Produce an image\n",
    "cfg_scaled_model = CFGScaledModel(model=model)\n",
    "cfg_scaled_model.train(False)\n",
    "\n",
    "solver = ODESolver(velocity_model=cfg_scaled_model)\n",
    "ode_opts = {\"step_size\": 0.01, \"nfe\": 50}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-paris",
   "metadata": {},
   "source": [
    "# Generate an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-retail",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, labels = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-shirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_shape = samples.shape\n",
    "sample_channels = samples_shape[1]\n",
    "sample_img_width = samples_shape[2]\n",
    "sample_img_height = samples_shape[3]\n",
    "\n",
    "print(\n",
    "    samples_shape,\n",
    "    sample_channels,\n",
    "    sample_img_width,\n",
    "    sample_img_height\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-professor",
   "metadata": {},
   "outputs": [],
   "source": [
    "edm_schedule = True\n",
    "\n",
    "# Continuous sampling\n",
    "x_0 = torch.randn((1, sample_channels, sample_img_width, sample_img_height), dtype=torch.float32, device=device)\n",
    "\n",
    "if edm_schedule:\n",
    "    time_grid = get_time_discretization(nfes=ode_opts[\"nfe\"])\n",
    "    \n",
    "else:\n",
    "    time_grid = torch.tensor([0.0, 1.0], device=device)\n",
    "\n",
    "synthetic_samples = solver.sample(\n",
    "    time_grid=time_grid,\n",
    "    x_init=x_0,\n",
    "    method=\"heun2\",\n",
    "    return_intermediates=False,\n",
    "    atol=ode_opts[\"atol\"] if \"atol\" in ode_opts else 1e-5,\n",
    "    rtol=ode_opts[\"rtol\"] if \"atol\" in ode_opts else 1e-5,\n",
    "    step_size=ode_opts[\"step_size\"]\n",
    "    if \"step_size\" in ode_opts\n",
    "    else None,\n",
    "    label={},  # No labels for our task\n",
    "    cfg_scale=0.0,  # 0.0 is unconditional and 1.0 is conditional on the label\n",
    ")\n",
    "\n",
    "# Scaling to [0, 1] from [-1, 1]\n",
    "synthetic_samples = torch.clamp(\n",
    "    synthetic_samples * 0.5 + 0.5, min=0.0, max=1.0\n",
    ")\n",
    "synthetic_samples = torch.floor(synthetic_samples * 255)\n",
    "synthetic_samples = synthetic_samples.to(torch.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-costs",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_img = (\n",
    "    synthetic_samples\n",
    "    .detach()\n",
    "    .cpu()\n",
    "    .squeeze()\n",
    "    .numpy()\n",
    ")\n",
    "\n",
    "synthetic_img = (\n",
    "    synthetic_img\n",
    "    .transpose(2, 1, 0)\n",
    "    .transpose(1, 0, 2)\n",
    ")\n",
    "\n",
    "plt.imshow(synthetic_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-injection",
   "metadata": {},
   "source": [
    "# Inverse Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-reduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'measurement': {\n",
    "        'operator': {'name': 'inpainting'},\n",
    "        'mask_opt': {'mask_type': 'random', 'mask_prob_range': (0.4, 0.4), 'image_size': 32},\n",
    "        'noise': {'name': 'gaussian', 'sigma': 0.01}\n",
    "    }\n",
    "}\n",
    "\n",
    "measure_config = config[\"measurement\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-calgary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test samples\n",
    "test_samples, test_labels = next(iter(test_loader))\n",
    "\n",
    "# Create the reference image\n",
    "ref_img = test_samples[0, :, :, :].squeeze()\n",
    "ref_numpy = ref_img\n",
    "\n",
    "print(ref_img.shape, ref_numpy.shape)\n",
    "\n",
    "ref_img = ref_img * 2.0 - 1.0\n",
    "ref_img = torch.tensor(ref_img)\n",
    "ref_img = (\n",
    "    ref_img\n",
    "    .unsqueeze(0)\n",
    "    .to(device)\n",
    ")\n",
    "print(ref_img.min(), ref_img.max(), ref_img.device)\n",
    "\n",
    "# Initalize operator\n",
    "operator = get_operator(device=device, **measure_config['operator'])\n",
    "noiser = get_noise(**measure_config['noise'])\n",
    "\n",
    "# For this case we will add a mask\n",
    "# and then noise\n",
    "mask_gen = mask_generator(\n",
    "    **measure_config['mask_opt']\n",
    ")\n",
    "mask = mask_gen(ref_img)\n",
    "mask = mask[:, 0, :, :].unsqueeze(dim=0)\n",
    "\n",
    "# Forward measurement model (Ax + n)\n",
    "y = operator.forward(ref_img, mask=mask)\n",
    "y_n = noiser(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-intent",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-locator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at y_n\n",
    "# map back to [0, 1]\n",
    "noised_img = (y_n + 1) / 2\n",
    "noised_img = (\n",
    "    noised_img\n",
    "    .detach()\n",
    "    .cpu()\n",
    "    .squeeze()\n",
    "    .numpy()\n",
    "    .transpose(1, 2, 0)\n",
    ")\n",
    "\n",
    "plt.imshow(noised_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-holder",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = y_n.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-imagination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "lr = 1e-2\n",
    "\n",
    "# FMPlug\n",
    "z = torch.randn(\n",
    "    (1, 3, image_size, image_size),\n",
    "    device=device,\n",
    "    dtype=torch.float32,\n",
    "    requires_grad=True\n",
    ")\n",
    "print(z.shape)\n",
    "criterion = torch.nn.MSELoss().to(device)\n",
    "params_group1 = {'params': z, 'lr': lr}\n",
    "\n",
    "optimizer = torch.optim.AdamW([params_group1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500 # SR, inpainting: 5,000, nonlinear deblurring: 10,000\n",
    "\n",
    "psnrs = []\n",
    "losses = []\n",
    "best_images = []\n",
    "\n",
    "if edm_schedule:\n",
    "    # small number of evaluations\n",
    "    time_grid = get_time_discretization(nfes=5)\n",
    "    \n",
    "else:\n",
    "    time_grid = torch.tensor([0.0, 1.0], device=device)\n",
    "\n",
    "for iterator in tqdm.tqdm(range(epochs)):\n",
    "    model.eval()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Iterate over the path\n",
    "    x_t = solver.sample(\n",
    "        time_grid=time_grid,\n",
    "        x_init=z,\n",
    "        method=\"heun2\",\n",
    "        return_intermediates=False,\n",
    "        enable_grad=True,\n",
    "        atol=ode_opts[\"atol\"] if \"atol\" in ode_opts else 1e-5,\n",
    "        rtol=ode_opts[\"rtol\"] if \"atol\" in ode_opts else 1e-5,\n",
    "        step_size=ode_opts[\"step_size\"]\n",
    "        if \"step_size\" in ode_opts\n",
    "        else None,\n",
    "        label={},\n",
    "        cfg_scale=0.0,  # 0.0 is unconditional and 1.0 is conditional on the label\n",
    "    )\n",
    "\n",
    "    # Make sure values are between -1 and 1\n",
    "    output = torch.clamp(x_t, -1, 1)\n",
    "    if measure_config['operator']['name'] == 'inpainting':\n",
    "        # In the inverse problem we want to estimate the noise operator\n",
    "        loss = criterion(operator.forward(output, mask=mask), y_n)\n",
    "        \n",
    "    else:\n",
    "        loss = criterion(operator.forward(output), y_n)\n",
    "        \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # Evaluate\n",
    "    with torch.no_grad():\n",
    "        output_numpy = output.detach().cpu().squeeze().numpy()\n",
    "        output_numpy = (output_numpy + 1) / 2\n",
    "        output_numpy = np.transpose(output_numpy, (1, 2, 0))  # Keep out for now lets evaluate\n",
    "        \n",
    "        # calculate psnr\n",
    "        tmp_psnr = peak_signal_noise_ratio(\n",
    "            ref_numpy.numpy().transpose(1, 2, 0),\n",
    "            output_numpy\n",
    "        )\n",
    "        psnrs.append(tmp_psnr)\n",
    "        \n",
    "        if len(psnrs) == 1 or (len(psnrs) > 1 and tmp_psnr > np.max(psnrs[:-1])):\n",
    "            best_img = output_numpy\n",
    "            best_images.append(best_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-galaxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_ref_img = (\n",
    "    ref_numpy\n",
    "    .detach()\n",
    "    .cpu()\n",
    "    .squeeze()\n",
    "    .numpy()\n",
    "    .transpose(1, 2, 0)\n",
    ")\n",
    "\n",
    "plt.imshow(display_ref_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-transport",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_img = best_images[-1]\n",
    "best_img = (\n",
    "    best_img\n",
    ")\n",
    "best_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imposed-gather",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 15))\n",
    "\n",
    "ax1.imshow(display_ref_img)\n",
    "ax1.set_title(\"Ground Truth Image\")\n",
    "\n",
    "ax2.imshow(noised_img)\n",
    "ax2.set_title(\"Noised / Inpainting Image\")\n",
    "\n",
    "ax3.imshow(best_img)\n",
    "ax3.set_title(\"Reconstructed Image (FM)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-lambda",
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_trajectory = pd.Series(psnrs)\n",
    "psnr_trajectory.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-playback",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "materialmind",
   "language": "python",
   "name": "materialmind"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
